{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "character_level.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ASXNQlX9rvhw",
        "colab_type": "code",
        "outputId": "17478ced-5281-451b-c431-bd798a227440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.layers import Dense,LSTM,Activation,Input,Embedding,Dropout,Bidirectional\n",
        "from keras.models import Model\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "! pip install keras_self_attention\n",
        "from keras_self_attention import SeqSelfAttention"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_self_attention\n",
            "  Downloading https://files.pythonhosted.org/packages/44/3e/eb1a7c7545eede073ceda2f5d78442b6cad33b5b750d7f0742866907c34b/keras-self-attention-0.42.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras_self_attention) (1.18.4)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras_self_attention) (2.3.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras_self_attention) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_self_attention) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_self_attention) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_self_attention) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_self_attention) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras_self_attention) (3.13)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.42.0-cp36-none-any.whl size=17296 sha256=04af2f02611d8af4b1bc969e23d7481a72489f4673a9560d823d16fac25add35\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/05/a0/99c0cf60d383f0494e10eca2b238ea98faca9a1fe03cac2894\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.42.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzbtu8jMrvhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data= open('/content/drive/My Drive/Colab Notebooks/sherlock.txt','r',encoding=\"utf8\").read().lower().split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiesqZbmrvh1",
        "colab_type": "code",
        "outputId": "367e8071-a709-42e7-c3a3-d50dc2ff0a92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for i in range(0,len(data)):\n",
        "    data[i] = ''.join(e for e in data[i] if e.isalnum())  ## remove special characters\n",
        "    data[i] = ''.join([i for i in data[i] if not i.isdigit()])  # remove numbers\n",
        "len(data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "107406"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVNi3YxCrvh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r',encoding=\"utf8\") as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIZT8UQLrvh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('/content/drive/My Drive/Colab Notebooks/glove.6B.100d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfXKAKoTrvh7",
        "colab_type": "code",
        "outputId": "ac331c59-da5b-4c12-cbb3-ac7931fde44b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# removing the words from data which are not in embedding\n",
        "for i in range(0,10):\n",
        "    for w in data:\n",
        "        try:\n",
        "            if(word_to_index[w]):\n",
        "                None\n",
        "        except:\n",
        "            data.remove(w)\n",
        "        \n",
        "len(data)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100319"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GphRPMk4rvh9",
        "colab_type": "code",
        "outputId": "7dea9a5e-7e2b-44c7-b411-d99609c5f1be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "unique_word=np.unique(data)\n",
        "print('unique_words:-',unique_word.shape[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique_words:- 7654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXuOska2rviA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Vocabulary\n",
        "dic = {}\n",
        "for w in unique_word:\n",
        "    dic[w]=word_to_vec_map[w]/3.7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpqaXtj2rviC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_ix = { ch:i for i,ch in enumerate(unique_word) }\n",
        "ix_to_word = { i:ch for i,ch in enumerate(unique_word) }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "dJC_Wy1ErviE",
        "colab_type": "code",
        "outputId": "2ffcab9c-5415-452e-bef1-5a8a0afb589f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "words_length = 10\n",
        "prev_word = []\n",
        "next_word = []\n",
        "for i in range(0,len(data)-words_length):\n",
        "    prev_word.append(data[i:i+words_length])\n",
        "    next_word.append(data[i+words_length])\n",
        "print('No of samples : ',len(prev_word))  # data size"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of samples :  100309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwtD4x3zrviG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_to_indices(prev_word,next_word,word_to_index,word_length):\n",
        "    m=len(prev_word)\n",
        "    X=np.zeros((m,word_length))\n",
        "    Y=np.zeros((m,1))\n",
        "    for i in range(0,m):\n",
        "        j=0\n",
        "#         print(i,end=\" \")\n",
        "        for w in prev_word[i]:\n",
        "            X[i,j]=word_to_index[w]\n",
        "            j=j+1\n",
        "        Y[i,0]=word_to_index[next_word[i]]\n",
        "            \n",
        "    return X,Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co4ZDaChrviI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_data,Y_data = sentence_to_indices(prev_word,next_word,word_to_ix,words_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP5iP6-wrviK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_onehot=np.zeros((len(Y_data),len(unique_word)),dtype=bool)\n",
        "for i in range(0,len(Y_data)):\n",
        "    Y_onehot[i,int(Y_data[i])]=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFD1h3uLrviN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train test split\n",
        "num=90000\n",
        "X_train = X_data[:num]\n",
        "Y_train = Y_onehot[:num]\n",
        "X_test  = X_data[num:]\n",
        "Y_test  = Y_onehot[num:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-fGnh5ArviP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_input=Input(shape=(words_length,))\n",
        "x=Embedding(input_dim=unique_word.shape[0]+1,output_dim=100)(x_input)\n",
        "x=Bidirectional(LSTM(128,return_sequences=True))(x)\n",
        "x=SeqSelfAttention(attention_activation='sigmoid')(x)\n",
        "x=LSTM(128)(x)\n",
        "x=Dropout(0.2)(x)\n",
        "x=Dense(int(unique_word.shape[0]/2),activation='relu',kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "x=Dense(unique_word.shape[0])(x)\n",
        "x=Activation('softmax')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii8_Qi1erviU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=Model(x_input,x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt0qx3bjrviX",
        "colab_type": "code",
        "outputId": "20dd7933-0c4f-497b-cf6a-ed85cfd9702b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 10, 100)           765500    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 10, 256)           234496    \n",
            "_________________________________________________________________\n",
            "seq_self_attention_1 (SeqSel (None, 10, 256)           16449     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3827)              493683    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 7654)              29299512  \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 7654)              0         \n",
            "=================================================================\n",
            "Total params: 31,006,760\n",
            "Trainable params: 31,006,760\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf0Mz6sGrviZ",
        "colab_type": "code",
        "outputId": "0d135852-965e-4812-92d1-fad687b5507e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#\n",
        "for l in model.layers:\n",
        "    if 'embedding' in l.name:\n",
        "        a=model.get_layer(l.name)\n",
        "        b=a.get_weights()\n",
        "        c=b[0]\n",
        "        i=0\n",
        "        for w in dic:\n",
        "            c[i]=dic[w]\n",
        "            i=i+1\n",
        "        b[0]=c\n",
        "        a.set_weights(b)\n",
        "        print('done')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECDzvvaIrvib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for l in model.layers:\n",
        "    if 'embedding' in l.name:\n",
        "        l.trainable=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8V34fEarvic",
        "colab_type": "code",
        "outputId": "2774d28c-ccf5-47ef-8cbc-133a7bceed4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "for l in model.layers:\n",
        "    print(l.name,l.trainable)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_1 False\n",
            "embedding_1 False\n",
            "bidirectional_1 True\n",
            "seq_self_attention_1 True\n",
            "lstm_2 True\n",
            "dropout_1 True\n",
            "dense_1 True\n",
            "dense_2 True\n",
            "activation_1 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIGjBD0Irvie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "los=tf.keras.losses.CategoricalCrossentropy()\n",
        "opt = keras.optimizers.Adam(lr=0.001)\n",
        "model.compile(loss=los,optimizer=opt,metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9Yy00sfrvig",
        "colab_type": "code",
        "outputId": "d90ffdc0-aace-4f1a-ecec-e6ff3a77601b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train,Y_train,epochs=50,batch_size=1024,shuffle=True)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "90000/90000 [==============================] - 15s 172us/step - loss: 2.2887 - accuracy: 0.4921\n",
            "Epoch 2/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 2.0876 - accuracy: 0.5334\n",
            "Epoch 3/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 2.0049 - accuracy: 0.5524\n",
            "Epoch 4/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 1.9533 - accuracy: 0.5668\n",
            "Epoch 5/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 1.9163 - accuracy: 0.5740\n",
            "Epoch 6/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 1.8848 - accuracy: 0.5850\n",
            "Epoch 7/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 1.8541 - accuracy: 0.5911\n",
            "Epoch 8/50\n",
            "90000/90000 [==============================] - 14s 159us/step - loss: 1.8248 - accuracy: 0.6003\n",
            "Epoch 9/50\n",
            "90000/90000 [==============================] - 14s 159us/step - loss: 1.8067 - accuracy: 0.6042\n",
            "Epoch 10/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 1.7867 - accuracy: 0.6114\n",
            "Epoch 11/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 1.7726 - accuracy: 0.6137\n",
            "Epoch 12/50\n",
            "90000/90000 [==============================] - 14s 159us/step - loss: 1.7549 - accuracy: 0.6198\n",
            "Epoch 13/50\n",
            "90000/90000 [==============================] - 15s 161us/step - loss: 1.7421 - accuracy: 0.6223\n",
            "Epoch 14/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 1.7227 - accuracy: 0.6262\n",
            "Epoch 15/50\n",
            "90000/90000 [==============================] - 14s 161us/step - loss: 1.7102 - accuracy: 0.6312\n",
            "Epoch 16/50\n",
            "90000/90000 [==============================] - 15s 162us/step - loss: 1.6962 - accuracy: 0.6352\n",
            "Epoch 17/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 1.6881 - accuracy: 0.6363\n",
            "Epoch 18/50\n",
            "90000/90000 [==============================] - 14s 158us/step - loss: 1.6753 - accuracy: 0.6390\n",
            "Epoch 19/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 1.6622 - accuracy: 0.6435\n",
            "Epoch 20/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 1.6528 - accuracy: 0.6465\n",
            "Epoch 21/50\n",
            "90000/90000 [==============================] - 15s 161us/step - loss: 1.6419 - accuracy: 0.6475\n",
            "Epoch 22/50\n",
            "90000/90000 [==============================] - 15s 162us/step - loss: 1.6374 - accuracy: 0.6483\n",
            "Epoch 23/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 1.6266 - accuracy: 0.6535\n",
            "Epoch 24/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 1.6135 - accuracy: 0.6559\n",
            "Epoch 25/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 1.6062 - accuracy: 0.6590\n",
            "Epoch 26/50\n",
            "90000/90000 [==============================] - 14s 161us/step - loss: 1.5982 - accuracy: 0.6602\n",
            "Epoch 27/50\n",
            "90000/90000 [==============================] - 14s 161us/step - loss: 1.5900 - accuracy: 0.6620\n",
            "Epoch 28/50\n",
            "90000/90000 [==============================] - 15s 163us/step - loss: 1.5826 - accuracy: 0.6633\n",
            "Epoch 29/50\n",
            "90000/90000 [==============================] - 14s 161us/step - loss: 1.5747 - accuracy: 0.6676\n",
            "Epoch 30/50\n",
            "90000/90000 [==============================] - 14s 161us/step - loss: 1.5648 - accuracy: 0.6683\n",
            "Epoch 31/50\n",
            "90000/90000 [==============================] - 14s 159us/step - loss: 1.5648 - accuracy: 0.6681\n",
            "Epoch 32/50\n",
            "90000/90000 [==============================] - 14s 159us/step - loss: 1.5539 - accuracy: 0.6716\n",
            "Epoch 33/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 1.5501 - accuracy: 0.6722\n",
            "Epoch 34/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 1.5375 - accuracy: 0.6747\n",
            "Epoch 35/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 1.5322 - accuracy: 0.6762\n",
            "Epoch 36/50\n",
            "90000/90000 [==============================] - 15s 161us/step - loss: 1.5296 - accuracy: 0.6784\n",
            "Epoch 37/50\n",
            "90000/90000 [==============================] - 15s 162us/step - loss: 1.5203 - accuracy: 0.6799\n",
            "Epoch 38/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 1.5159 - accuracy: 0.6820\n",
            "Epoch 39/50\n",
            "90000/90000 [==============================] - 14s 159us/step - loss: 1.5103 - accuracy: 0.6822\n",
            "Epoch 40/50\n",
            "90000/90000 [==============================] - 14s 161us/step - loss: 1.5020 - accuracy: 0.6856\n",
            "Epoch 41/50\n",
            "90000/90000 [==============================] - 14s 161us/step - loss: 1.4977 - accuracy: 0.6858\n",
            "Epoch 42/50\n",
            "90000/90000 [==============================] - 15s 162us/step - loss: 1.4925 - accuracy: 0.6861\n",
            "Epoch 43/50\n",
            "90000/90000 [==============================] - 15s 162us/step - loss: 1.4826 - accuracy: 0.6902\n",
            "Epoch 44/50\n",
            "90000/90000 [==============================] - 14s 161us/step - loss: 1.4833 - accuracy: 0.6896\n",
            "Epoch 45/50\n",
            "90000/90000 [==============================] - 15s 161us/step - loss: 1.4722 - accuracy: 0.6922\n",
            "Epoch 46/50\n",
            "90000/90000 [==============================] - 15s 162us/step - loss: 1.4660 - accuracy: 0.6931\n",
            "Epoch 47/50\n",
            "90000/90000 [==============================] - 14s 161us/step - loss: 1.4599 - accuracy: 0.6971\n",
            "Epoch 48/50\n",
            "90000/90000 [==============================] - 14s 159us/step - loss: 1.4575 - accuracy: 0.6961\n",
            "Epoch 49/50\n",
            "90000/90000 [==============================] - 14s 160us/step - loss: 1.4528 - accuracy: 0.6966\n",
            "Epoch 50/50\n",
            "90000/90000 [==============================] - 14s 161us/step - loss: 1.4429 - accuracy: 0.6995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f91ffde5e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z484-eTprvil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/content/drive/My Drive/Colab Notebooks/next_word.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9W9OzWZrvis",
        "colab_type": "code",
        "outputId": "8faaa4eb-bc99-41b9-f236-429a133fdd78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "x = X_test[900]\n",
        "t=np.zeros((1,41))\n",
        "for w in x:\n",
        "    print(ix_to_word[w],\" \")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is  \n",
            "something  \n",
            "distinctly  \n",
            "novel  \n",
            "about  \n",
            "some  \n",
            "of  \n",
            "the  \n",
            "features  \n",
            "if  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHfEI2yVrvit",
        "colab_type": "code",
        "outputId": "74e15803-0bd9-43cd-caba-a0838e3f3a50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "#predicting next 30 words \n",
        "for i in range(0,30):\n",
        "    y = model.predict(np.reshape(x,(1,10)))\n",
        "    y = np.argmax(y)\n",
        "    print(ix_to_word[y],\" \")\n",
        "    for i in range(0,len(x)-1):\n",
        "        x[i]=x[i+1]\n",
        "    x[9]=y"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lord  \n",
            "st  \n",
            "simon  \n",
            "i  \n",
            "have  \n",
            "given  \n",
            "you  \n",
            "a  \n",
            "better  \n",
            "good  \n",
            "i  \n",
            "understand  \n",
            "that  \n",
            "i  \n",
            "have  \n",
            "been  \n",
            "able  \n",
            "to  \n",
            "tell  \n",
            "you  \n",
            "i  \n",
            "knew  \n",
            "that  \n",
            "i  \n",
            "had  \n",
            "done  \n",
            "to  \n",
            "be  \n",
            "able  \n",
            "to  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7Xnt35Zrviy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}